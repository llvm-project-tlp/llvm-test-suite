#!/usr/bin/env python
#
# Analyze the report produced by running the test suite and print some summary
# statistics. This compares the report to the static test configuration. If run
# on a report generated by forcing all the tests to be run, this can identify
# tests that are in the DisabledFiles.cmake lists but now pass.

import argparse
import glob
import json
import os
import re
import typing

def ancestor(n, p):
    if n:
        return ancestor(n - 1, os.path.dirname(p))
    return p

def parse_test_name(s: str) -> str:
    # The test name is expected to start with Fortran/gfortran and end with
    # .test
    # FIXME: This probably depends on how the test suite is run. There might
    # be more text before Fortran/gfortran in which case we must get rid of
    # that as well.
    s = s.replace('test-suite :: Fortran/gfortran/', '')
    s = s.replace('.test', '')

    # This will be followed by the subdirectory in which the actual test is
    # The file name will be some mangled name dependent on the subdirectory
    # path.
    filename = os.path.basename(s)
    subdirs = s.replace(filename, '')

    # The filename starts with gfortran-$1-$2- where $1 \in
    # {regression,torture} and $2 \in {compile,execute}.
    for prefix in [
        'gfortran-regression-compile-',
        'gfortran-regression-execute-',
        'gfortran-torture-compile-',
        'gfortran-torture-execute-'
    ]:
        filename = filename.replace(prefix, '', 1)

    # The path separators in the subdirs are replaced with __ in the mangled
    # filename. Undo that to get the relative path to the failing test.
    filename = filename.replace(subdirs.replace('/', '__'), subdirs, 1)

    # The last underscore in the filename will be that of the extension of the
    # source file.
    left, _, right = filename.rpartition('_')
    filename = '.'.join([left, right])

    # Some files regression/gomp/appendix-a use dots in the filename which are
    # replaced with underscores in the mangled name. Special case those.
    if filename.startswith('regression/gomp/appendix-a'):
        filename = filename.replace('_', '.')

    return filename

def parse_report(root, filename):
    def check_if_exists(f):
        path = os.path.join(root, 'Fortran', 'gfortran', f)
        if not os.path.exists(path):
            print('ERROR: Could not find file:', f)
            exit(1)

    passing = []
    failing = []
    with open(filename) as f:
        # FIXME: Maybe filter this so we only return the tests from the gfortran
        # test suite since the report (particularly the default) may have been
        # run for all the Fortran tests.
        j = json.load(f)
        for t in j['tests']:
            if t['code'] == 'PASS':
                passing.append(parse_test_name(t['name']))
            elif t['code'] == 'FAIL' or t['code'] == 'NOEXE':
                failing.append(parse_test_name(t['name']))

    # Check that the files are in the source for sanity.
    for f in passing:
        check_if_exists(f)

    for f in failing:
        check_if_exists(f)

    return sorted(passing), sorted(failing)

def parse_test_config(source, filename):
    dirname = os.path.dirname(filename).replace('Fortran/gfortran/', '')
    tests = []
    with open(os.path.join(source, filename)) as f:
        for line in f:
            l = line.strip()
            if not l or l.startswith('#'):
                continue

            # The line consists of fields separated by semicolons. The second
            # field is a space separated list of files that constitute a single
            # test. If there is more than one file in the list, the first
            # file is the main test file from which the name of the test will
            # be derived.
            fields = l.split(';')
            main = fields[1].split(' ')[0]
            tests.append(os.path.join(dirname, main))

    return sorted(tests)

def parse_test_configs(source):
    configs = glob.glob(
        'Fortran/gfortran/**/tests.cmake', root_dir = source, recursive = True
    )
    tests = []
    for config in configs:
        tests.extend(parse_test_config(source, config))

    return sorted(tests)

def parse_disabled(source, filename):
    re_unsupported = re.compile('^file[ ]*[(]GLOB[ ]+UNSUPPORTED_FILES.*')
    re_unimplemented = re.compile('^file[ ]*[(]GLOB[ ]+UNIMPLEMENTED_FILES.*')
    re_erroring = re.compile('file[ ]*[(]GLOB[ ]+SKIPPED_FILES.*')
    re_failing = re.compile('file[ ]*[(]GLOB[ ]+FAILING_FILES.*')

    dirname = os.path.dirname(filename).replace('Fortran/gfortran/', '')
    with open(os.path.join(source, filename)) as f:
        unsupported = []
        unimplemented = []
        erroring = []
        failing = []
        which = None
        for line in f:
            l = line.strip()
            if not l or l.startswith('#'):
                continue

            # Strip trailing comments from the line.
            idx = l.find('#')
            if idx != -1:
                l = l[:idx].strip()

            if re_unsupported.match(l):
                which = unsupported
            elif re_unimplemented.match(l):
                which = unimplemented
            elif re_erroring.match(l):
                which = erroring
            elif re_failing.match(l):
                which = failing
            elif which is not None and l.endswith(')'):
                rest = l[:-1]
                if rest:
                    which.append(os.path.join(dirname, rest))
                which = None
            elif which is not None:
                which.append(os.path.join(dirname, l))

        return (unsupported, unimplemented, erroring, failing)

def parse_disabled_files(source):
    files = glob.glob(
        'Fortran/gfortran/**/DisabledFiles.cmake',
        root_dir = source,
        recursive = True
    )
    unsupported = []
    unimplemented = []
    erroring = []
    failing = []
    for f in sorted(files):
        uns, uni, err, fail = parse_disabled(source, f)
        unsupported.extend(sorted(uns))
        unimplemented.extend(sorted(uni))
        erroring.extend(sorted(err))
        failing.extend(sorted(fail))

    return (unsupported, unimplemented, erroring, failing)

def parse_commandline_args():
    ap = argparse.ArgumentParser(
        description = 'Analyze the report obtained by forcing all the '
        'gfortran tests to run, even if they have been explicitly disabled'
    )

    ap.add_argument(
        '-v',
        '--verbose',
        action = 'store_true',
        help = 'Show details about the newly passing tests'
    )

    ap.add_argument('report', help = 'The report produced by lit')

    return ap.parse_args()

def main():
    args = parse_commandline_args()
    root = ancestor(4, os.path.realpath(__file__))

    tests = parse_test_configs(root)
    unsupported, unimplemented, erroring, failing = \
        parse_disabled_files(root)

    disabled = []
    disabled.extend(unsupported)
    disabled.extend(unimplemented)
    disabled.extend(erroring)
    disabled.extend(failing)

    passing, not_passing = parse_report(root, args.report)

    # These are tests that are in the configuration but do not appear in the
    # report because they are irrelevant to flang (such as all the tests in
    # regression/graphite, regression/guality and regression/prof)
    excluded_everywhere = []

    # These are tests that are in the configuration but do not appear in the
    # report because they are not enabled on the target on which the tests
    # were run.
    excluded_here = []
    for t in tests:
        # The tests in some directories are ignored altogether because they
        # are not relevant. Don't count those in the skipped tests.
        if t.startswith('regression/graphite') or \
           t.startswith('regression/guality') or \
           t.startswith('regression/prof'):
            excluded_everywhere.append(t)
            continue
        elif t in passing or t in not_passing or t in disabled:
            continue
        else:
            excluded_here.append(t)

    # If the number of tests executed are exactly the same as the number of
    # tests in the test configuration, then we have forced all the test to
    # run. In this case, carry out a sanity check. Every file in the disabled
    # lists must either pass, fail or be explicitly excluded.
    if len(tests) == len(passing) + len(failing):
        for t in disabled:
            if t not in not_passing and \
               t not in passing and \
               t not in excluded_everywhere and \
               t not in excluded_here:
                print('ERROR: Unexpected test in disabled list:', t)

    # If the report that was passed was produced by forcing all the tests to
    # be run, there may be files that are in the disabled files lists that now
    # pass. These are the newly passing tests which should be removed from the
    # disabled lists manually.
    newly_passing = []
    for t in passing:
        if t in disabled:
            newly_passing.append(t)

    # The viable tests that are not excluded or in the "unsupported" list. The
    # latter consists of tests that test non-standard extensions or
    # gfortran-specific behavior that flang currently does not attempt to
    # replicate. This number is essentially the number of tests that ought to
    # eventually pass.
    viable = len(tests) \
        - len(excluded_everywhere) \
        - len(excluded_here) \
        - len(unsupported)

    print('Summary')
    print('  Total: {}'.format(len(tests)))
    print('  Viable: {}'.format(viable))
    print('  Passing: {}'.format(len(passing)))
    print('    Can be enabled: {}'.format(len(newly_passing)))
    if args.verbose:
        g = {}
        for f in newly_passing:
            d = os.path.dirname(f)
            if d not in g:
                g[d] = []
            g[d].append(os.path.basename(f))

        groups = sorted(g.keys())
        for group in groups:
            print('      {}'.format(group))
            for f in g[group]:
                print('        {}'.format(f))

    print('  Failing:', len(not_passing))
    print('  Disabled:', len(disabled))
    print('    After enabling: {}'.format(len(disabled) - len(newly_passing)))
    print('  Ignored:', len(excluded_everywhere))
    print('  Excluded:', len(excluded_here))

    return 0

if __name__ == '__main__':
    exit(main())
